{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **#04. Topic Modeling**\n",
        "- Instructor: [Jaeung Sim](https://jaeungs.github.io/) (University of Connecticut)\n",
        "- Course: OPIM 5671 Data Mining and Time Series Forecasting\n",
        "- Last updated: February 11, 2025"
      ],
      "metadata": {
        "id": "AYaMkhHpiCmQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objectives**\n",
        "1. Understand topic modeling and its applications.\n",
        "1. Exercise topic modeling using Latent Dirichlet Allocation (LDA) in Python.\n",
        "\n",
        "**References**\n",
        "* [Topic model (Wikipedia)](https://en.wikipedia.org/wiki/Topic_model)\n",
        "* [A Deeper Meaning: Topic Modeling in Python](https://www.toptal.com/python/topic-modeling-python)\n",
        "* [Hands-On Topic Modeling with Python](https://towardsdatascience.com/hands-on-topic-modeling-with-python-1e3466d406d7)\n",
        "* [Disneyland Reviews at Kaggle (Data Source)](https://www.kaggle.com/datasets/arushchillar/disneyland-reviews)\n",
        "* [A friendly guide to NLP: Bag-of-Words with Python example](https://www.analyticsvidhya.com/blog/2021/08/a-friendly-guide-to-nlp-bag-of-words-with-python-example/)"
      ],
      "metadata": {
        "id": "LBJp_m7BogYB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Part 1. Conceptual Background**"
      ],
      "metadata": {
        "id": "JBXz9eX4wFwi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A. Theoretical background**\n",
        "\n",
        "A **topic model** is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body.\n",
        "\n",
        "**Latent Dirichlet Allocation (LDA)**\n",
        "\n",
        "LDA is a generative statistical model that explains a set of observations through unobserved groups, and each group explains why some parts of the data are similar. The LDA is a popular statistical unsupervised machine learning model for topic modeling. It assumes each topic is made up of words and each document (in our case each review) consists of a collection of these words. Therefore, LDA tries to find words that best describe each topic and matches reviews that are represented by these words.\n",
        "\n",
        "LDA uses Dirichlet distribution, a generalization of Beta distribution that models probability distribution for two or more outcomes ($K$). Dirichlet distribution denoted with $Dir(\\alpha)$ where $\\alpha < 1$ (symmetric) indicates sparsity, and it is exactly how we want to present topics and words for topic modeling. As you can see below, with $\\alpha < 1$ we have circles on sides/corners separated from each other (in other words sparse), and with $\\alpha > 1$ we have circles in the center very close to each other and difficult to distinguish. You can imagine these circles as topics.\n",
        "\n",
        "![image](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*DTtEsh9WDcC8sSPA6dZSIg.jpeg)\n",
        "\n",
        "LDA uses two Dirichlet distributions where\n",
        "* $K$ is the number of topics.\n",
        "* $M$ denotes the number of documents.\n",
        "* $N$ denotes the number of words in a given document.\n",
        "* $Dir(\\alpha)$ is the Dirichlet distribution per-document topic distribution.\n",
        "* $Dir(\\beta)$ is the Dirichlet distribution per-topic word distribution.\n",
        "* $\\theta_{i}$ is the topic distribution for document $i$.\n",
        "* $\\varphi_{k}$ is the word distribution for topic $k$.\n",
        "* $z_{ij}$ is the topic for the $j$-th word in document $i$.\n",
        "* $w_{ij}$ is the specific word.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QdEoZyh0qUr2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we bring all the pieces together, we get the formula below, which describes the probability of a document with two Dirichlet distributions followed by multinomial distributions.\n",
        "\n",
        "$P(\\boldsymbol{W}, \\boldsymbol{Z}, \\boldsymbol{\\theta}, \\boldsymbol{\\varphi}; \\alpha, \\beta) = \\prod_{k=1}^K P(\\varphi_k; \\beta) \\prod_{i=1}^M P(\\theta_i; \\alpha) \\prod_{j=1}^N P(Z_{ij} | \\theta_i) P(W_{ij} | \\varphi_{z_{ij}})$\n",
        "\n",
        "This suggests that you want to maximize the product of the three probability measures:\n",
        "* $\\prod_{k=1}^K P(\\varphi_k; \\beta)$: How accurate the predicted word distributions for topics are\n",
        "* $\\prod_{i=1}^M P(\\theta_i; \\alpha)$: How accurate the predicted topic distributions for documents are\n",
        "* $\\prod_{j=1}^N P(Z_{ij} | \\theta_i) P(W_{ij} | \\varphi_{z_{ij}})$: For the given distributions, how accurate the predicted co-occurrence probabilities of words and topics are"
      ],
      "metadata": {
        "id": "VEwxlgDrLcb0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**B. Examples of Real-world Applications**\n",
        "\n",
        "Please refer to the following papers:\n",
        "* Jing Gong, Vibhanshu Abhishek, Beibei Li (2018) \"Examining the Impact of Keyword Ambiguity on Search Advertising Performance: A Topic Modeling Approach,\" ***MIS Quarterly*** 42(3), pp. 805-829.\n",
        "* Jorge Mejia, Shawn Mankad, Anandasivam Gopal (2021) \"Service Quality Using Text Mining: Measurement and Consequences,\" ***Manufacturing & Service Operations Management*** 23(6), pp. 1354-1372.\n",
        "* David Ardia, Keven Bluteau, Kris Boudt, Koen Inghelbrecht (2022) \"Climate Change Concerns and the Performance of Green vs. Brown Stocks,\" ***Management Science***, forthcoming.\n",
        "\n"
      ],
      "metadata": {
        "id": "-hruH7R0BuBb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**C. Overview of Implementation**\n",
        "\n",
        "1. Loading relevant libraries\n",
        "1. Exploring data structures\n",
        "1. Text parsing and filtering\n",
        "1. Bag-of-Words\n",
        "1. Determining the number of topics\n"
      ],
      "metadata": {
        "id": "mv4ewirTqWe8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Part 2. Understanding the Data**"
      ],
      "metadata": {
        "id": "WOjDiXAbV_U1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Introduction to the Dataset**\n",
        "* **Source:** Disney Land Review Dataset at Kaggle (<https://www.kaggle.com/datasets/arushchillar/disneyland-reviews>)\n",
        "* **About this file**\n",
        "  * The dataset includes 42,000 reviews of 3 Disneyland branches - Paris, California and Hong Kong, posted by visitors on Trip Advisor. You can refer to https://www.kaggle.com/datasets/arushchillar/disneyland-reviews for more details.\n",
        "  * Column Description\n",
        "    1. `Review_ID`: unique id given to each review\n",
        "    1. `Rating`: ranging from 1 (unsatisfied) to 5 (satisfied)\n",
        "    1. `Year_Month`: when the reviewer visited the theme park\n",
        "    1. `Reviewer_Location`: country of origin of visitor\n",
        "    1. `Review_Text`: comments made by visitor\n",
        "    1. `Disneyland_Branch`: location of Disneyland Park"
      ],
      "metadata": {
        "id": "UKN6QJMHWIuU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Download data with Python codes**"
      ],
      "metadata": {
        "id": "gvqNU_ExWV9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Libraries for data downloading and processing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import kagglehub\n",
        "import os"
      ],
      "metadata": {
        "id": "UZSOc9ziWE8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Libraries for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "plt.style.use(\"fivethirtyeight\")\n",
        "pd.set_option('display.max_colwidth', 80)\n",
        "import matplotlib.patheffects as path_effects\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "jcNZFGvsbYXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"arushchillar/disneyland-reviews\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "LGWTgflvWDxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deal with DataFrame**"
      ],
      "metadata": {
        "id": "H4NJ7RC1WbDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List files in the downloaded dataset directory\n",
        "files = os.listdir(path)\n",
        "print(\"Files in dataset:\", files)\n",
        "\n",
        "# Load the CSV file (assuming there's only one CSV file)\n",
        "csv_file = [f for f in files if f.endswith('.csv')][0]  # Get the first CSV file\n",
        "csv_path = os.path.join(path, csv_file)"
      ],
      "metadata": {
        "id": "4SxwOWE8WdN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read into DataFrame with default encoding (UTF-8)\n",
        "df = pd.read_csv(csv_path) # Yield an error"
      ],
      "metadata": {
        "id": "wKKZFXLoWjgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Attempt with ISO-8859-1 encoding\n",
        "df = pd.read_csv(csv_path, encoding=\"ISO-8859-1\") # Or type: encoding=\"latin-1\" / encoding=\"Windows-1252\"\n",
        "df.head()"
      ],
      "metadata": {
        "id": "bFMUXQl1X_zL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "q_AL2HqmWkkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check data types\n",
        "df.info()"
      ],
      "metadata": {
        "id": "IoCbKT8QWmVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a bar plot with value counts\n",
        "sns.countplot(x='Rating', data=df)"
      ],
      "metadata": {
        "id": "wAlWiWM0b2JW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Part 3. Implementation**\n",
        "\n",
        "**Objectives**\n",
        "* Process text data with `nltk` library.\n",
        "* Draw text topics and determine their numbers.\n",
        "* Explore topics with `pyLDAvis`.\n",
        "\n",
        "**References**\n",
        "* [A Deeper Meaning: Topic Modeling in Python](https://www.toptal.com/python/topic-modeling-python)\n",
        "* [Hands-On Topic Modeling with Python](https://towardsdatascience.com/hands-on-topic-modeling-with-python-1e3466d406d7)\n",
        "* [Disneyland Reviews at Kaggle (Data Source)](https://www.kaggle.com/datasets/arushchillar/disneyland-reviews)\n",
        "* [Python | Lemmatization with NLTK](https://www.geeksforgeeks.org/python-lemmatization-with-nltk/)\n",
        "* [A friendly guide to NLP: Bag-of-Words with Python example](https://www.analyticsvidhya.com/blog/2021/08/a-friendly-guide-to-nlp-bag-of-words-with-python-example/)"
      ],
      "metadata": {
        "id": "MF_DwJw_wLPZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **3.1. Loading NLP Libraries**"
      ],
      "metadata": {
        "id": "ouPjer597Ol1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A. Natural Language Processing Tools**"
      ],
      "metadata": {
        "id": "-Q6qItIacj31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NLP libraries\n",
        "import nltk\n",
        "import gensim\n",
        "import string\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator"
      ],
      "metadata": {
        "id": "4N3BJySvwOkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "TLN2NmgtcjpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **3.2. Text Processing**"
      ],
      "metadata": {
        "id": "FIvEnzSWVH8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are a few additions to in earlier notebooks:\n",
        "* Dealing with contractions\n",
        "* Extending the stop word set by adding contextual terms"
      ],
      "metadata": {
        "id": "xnyDp_i4f1gu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Considering contractions in English**"
      ],
      "metadata": {
        "id": "tVB6EirxgcvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A dictionary of main contractions in English\n",
        "contractions = {\n",
        "\"ain't\": \"am not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how is\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"needn't\": \"need not\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there had\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we would\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who's\": \"who is\",\n",
        "\"won't\": \"will not\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"you'd\": \"you would\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you're\": \"you are\",\n",
        "\"you've\": \"you have\"\n",
        "}"
      ],
      "metadata": {
        "id": "oQ1lqqWrVILR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extending the stop word set by adding contextual terms**"
      ],
      "metadata": {
        "id": "QwlqGppHgvSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a basic stop word set\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "XIOL42-Rg-fR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extend the stop word set\n",
        "stop_words.update(['park', 'disney', 'disneyland']) # Context-specific stopwords"
      ],
      "metadata": {
        "id": "sbJ6cxSTg-ZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define a text processing function**"
      ],
      "metadata": {
        "id": "cpk7FGo95uLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bring lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "j5TQ6TM-g-Ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a text pre-processing function\n",
        "def process_text(text):\n",
        "    # Lowercasing\n",
        "    text = text.lower()\n",
        "\n",
        "    # Expand contractions\n",
        "    if True:\n",
        "        text = text.split()\n",
        "        new_text = []\n",
        "        for word in text:\n",
        "            if word in contractions:\n",
        "                new_text.append(contractions[word])\n",
        "            else:\n",
        "                new_text.append(word)\n",
        "        text = \" \".join(new_text)\n",
        "\n",
        "    # Remove punctuation and special characters\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\'', ' ', text)\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords & perform lemmatization\n",
        "    filtered_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "\n",
        "    return ' '.join(filtered_tokens)"
      ],
      "metadata": {
        "id": "Jjp6xaieg-Hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the pre-processing function\n",
        "df['Review_Clean'] = df['Review_Text'].apply(process_text)\n",
        "df['Review_Clean']"
      ],
      "metadata": {
        "id": "CCKxYGBf51_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Join text together\n",
        "review_words = ','.join(list(df['Review_Clean'].values))\n",
        "\n",
        "# Count each word\n",
        "Counter = Counter(review_words.split())\n",
        "most_frequent = Counter.most_common(30)\n",
        "\n",
        "# Bar plot of frequent words\n",
        "fig = plt.figure(1, figsize = (20,10))\n",
        "_ = pd.DataFrame(most_frequent, columns=(\"words\",\"count\"))\n",
        "sns.barplot(x = 'words', y = 'count', data = _, palette = 'winter')\n",
        "plt.xticks(rotation=45);"
      ],
      "metadata": {
        "id": "fcRLci1Rh3aN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the word cloud\n",
        "wordcloud = WordCloud(background_color=\"white\",\n",
        "                      max_words= 200,\n",
        "                      contour_width = 8,\n",
        "                      contour_color = \"steelblue\",\n",
        "                      collocations=False).generate(review_words)\n",
        "\n",
        "# Visualize the word cloud\n",
        "fig = plt.figure(1, figsize = (10, 10))\n",
        "plt.axis('off')\n",
        "plt.imshow(wordcloud)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s5LQeSNGh_wG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **3.3. Bag-of-Words**"
      ],
      "metadata": {
        "id": "RyBfqwjJiqhX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to use text as an input to machine learning algorithms, we need to present it in a numerical format. **Bag-of-words** is a vector space model and represents the occurrence of words in the document. In other words, bag-of-words converts each review into a collection of word counts without giving importance to the order or meaning.\n",
        "\n",
        "Here are a few example sentences about Game of Thrones:\n",
        "* Review 1: Game of Thrones is an amazing tv series!\n",
        "* Review 2: Game of Thrones is the best tv series!\n",
        "* Review 3: Game of Thrones is so great.\n",
        "\n",
        "Each row corresponds to a different review, while the rows are the unique words, contained in the three documents.\n",
        "\n",
        "![Image](https://cdn-images-1.medium.com/max/1000/1*cHKkqYIhaYuYwuuhBiSlHw.png)\n"
      ],
      "metadata": {
        "id": "y1nEEBNP7ZJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure 'Review_Clean_List' contains tokenized text (lists of words)\n",
        "df['Review_Clean_List'] = df['Review_Clean'].apply(lambda x: x.split() if isinstance(x, str) else x)\n"
      ],
      "metadata": {
        "id": "aL7WNd7FkKki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Review_Clean_List']"
      ],
      "metadata": {
        "id": "hFpXVkQ0kd3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Dictionary\n",
        "id2word = gensim.corpora.Dictionary(df['Review_Clean_List'])\n",
        "\n",
        "# Create Corpus: Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in df['Review_Clean_List']]"
      ],
      "metadata": {
        "id": "tZrOJKCMpzMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **3.4. Determining the Number of Topics**"
      ],
      "metadata": {
        "id": "Vf8WeuEx5Btg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deciding on the number of topics for the topic modeling can be difficult. Since we have initial knowledge of the context, determining the number of topics for modeling wouldn't be too outraging. However, if this number is too much then the model might fail to detect a topic that is actually broader and if this number is too less then topics might have large overlapping words. Because of these reasons, we will use the topic coherence score."
      ],
      "metadata": {
        "id": "Lc7_a27K8GKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute coherence score\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# By the number of topics from 1 to 7 (~1 min per each)\n",
        "number_of_topics = []\n",
        "coherence_score = []\n",
        "for i in range(1,8):\n",
        "  lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, iterations=50, num_topics=i)\n",
        "  coherence_model_lda = CoherenceModel(model=lda_model, texts=df['Review_Clean_List'], dictionary=id2word, coherence='c_v')\n",
        "  coherence_lda = coherence_model_lda.get_coherence()\n",
        "  number_of_topics.append(i)\n",
        "  coherence_score.append(coherence_lda);"
      ],
      "metadata": {
        "id": "GU_1z8uS5G1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataframe of coherence score by number of topics\n",
        "topic_coherence = pd.DataFrame({'number_of_topics':number_of_topics,\n",
        "                                'coherence_score':coherence_score})"
      ],
      "metadata": {
        "id": "5QWQO2Y_6DK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print a line plot\n",
        "sns.lineplot(data=topic_coherence, x='number_of_topics', y='coherence_score')"
      ],
      "metadata": {
        "id": "hsGEWzqL6Eko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore words occurring in each topic with their relative weight\n",
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print(\"Topic: {} Word: {}\".format(idx, topic))\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "sm1oDrDO5YRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **3.5. Getting Topic Weights with the Optimal Number**"
      ],
      "metadata": {
        "id": "tZCobdFqmnv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define LDA model\n",
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, iterations=50, num_topics=5)\n",
        "\n",
        "# Compute topic distributions for each document in the corpus\n",
        "lda_topics = [lda_model.get_document_topics(doc, minimum_probability=0) for doc in corpus]\n",
        "\n",
        "# Convert topic distributions into a DataFrame\n",
        "topic_weights = pd.DataFrame([[topic_prob for _, topic_prob in doc] for doc in lda_topics],\n",
        "                             columns=[f\"Topic_{i}\" for i in range(lda_model.num_topics)])\n",
        "\n",
        "# Merge topic weights with original DataFrame\n",
        "df_with_topics = pd.concat([df['Review_Clean_List'], topic_weights], axis=1)"
      ],
      "metadata": {
        "id": "GEVIVV1CBsAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the results\n",
        "df_with_topics"
      ],
      "metadata": {
        "id": "A0YJgq2GoOb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's create some variables with topics!**"
      ],
      "metadata": {
        "id": "1qxiYFG5o0bp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the necessary columns exist in the DataFrame\n",
        "topic_columns = ['Topic_0', 'Topic_1', 'Topic_2', 'Topic_3', 'Topic_4']\n",
        "\n",
        "# Compute mean and variance of the five topic columns\n",
        "df_with_topics['Topic_Mean'] = df_with_topics[topic_columns].mean(axis=1)\n",
        "df_with_topics['Topic_Variance'] = df_with_topics[topic_columns].var(axis=1)\n",
        "\n",
        "# Display the updated DataFrame\n",
        "df_with_topics"
      ],
      "metadata": {
        "id": "jMAjDvsnoyAg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}