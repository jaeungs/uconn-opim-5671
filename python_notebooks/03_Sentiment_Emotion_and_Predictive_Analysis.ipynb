{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **#03. Sentiment, Emotion, and Predictive Analysis with Text Data**\n",
        "- Instructor: [Jaeung Sim](https://jaeungs.github.io/) (University of Connecticut)\n",
        "- Course: OPIM 5671 Data Mining and Time Series Forecasting\n",
        "- Last updated: September 17, 2025"
      ],
      "metadata": {
        "id": "tZDUtwt2qJt3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objectives**\n",
        "1. Predict sentiment and other emotion variables using pre-trained models.\n",
        "1. Build a predictive model using sentiment and emotion variables.\n",
        "\n",
        "**References**\n",
        "* [Disneyland Reviews at Kaggle (Data Source)](https://www.kaggle.com/datasets/arushchillar/disneyland-reviews)\n",
        "* [Python | Lemmatization with NLTK](https://www.geeksforgeeks.org/python-lemmatization-with-nltk/)\n",
        "* [A friendly guide to NLP: Bag-of-Words with Python example](https://www.analyticsvidhya.com/blog/2021/08/a-friendly-guide-to-nlp-bag-of-words-with-python-example/)"
      ],
      "metadata": {
        "id": "GsIT94k2qZvy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Part 1. Understanding the Data**"
      ],
      "metadata": {
        "id": "kYWG1hbuqyif"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Introduction to the Dataset**\n",
        "* **Source:** Disney Land Review Dataset at Kaggle (<https://www.kaggle.com/datasets/arushchillar/disneyland-reviews>)\n",
        "* **About this file**\n",
        "  * The dataset includes 42,000 reviews of 3 Disneyland branches - Paris, California and Hong Kong, posted by visitors on Trip Advisor. You can refer to https://www.kaggle.com/datasets/arushchillar/disneyland-reviews for more details.\n",
        "  * Column Description\n",
        "    1. `Review_ID`: unique id given to each review\n",
        "    1. `Rating`: ranging from 1 (unsatisfied) to 5 (satisfied)\n",
        "    1. `Year_Month`: when the reviewer visited the theme park\n",
        "    1. `Reviewer_Location`: country of origin of visitor\n",
        "    1. `Review_Text`: comments made by visitor\n",
        "    1. `Disneyland_Branch`: location of Disneyland Park"
      ],
      "metadata": {
        "id": "ycZnLKNiq0cg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Download data with Python codes**"
      ],
      "metadata": {
        "id": "NEyT-ZHjq3Jc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nC0yeJoqqGUS"
      },
      "outputs": [],
      "source": [
        "# Libraries for data downloading and processing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import kagglehub\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Libraries for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "plt.style.use(\"fivethirtyeight\")\n",
        "pd.set_option('display.max_colwidth', 80)\n",
        "import matplotlib.patheffects as path_effects\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "7cMdj4MDq6Hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"arushchillar/disneyland-reviews\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "EOl0Nktwq7NU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deal with DataFrame**"
      ],
      "metadata": {
        "id": "LT1yT4Geq9ug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List files in the downloaded dataset directory\n",
        "files = os.listdir(path)\n",
        "print(\"Files in dataset:\", files)\n",
        "\n",
        "# Load the CSV file (assuming there's only one CSV file)\n",
        "csv_file = [f for f in files if f.endswith('.csv')][0]  # Get the first CSV file\n",
        "csv_path = os.path.join(path, csv_file)"
      ],
      "metadata": {
        "id": "6bI4Bz7-q-ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read into DataFrame with default encoding (UTF-8)\n",
        "df = pd.read_csv(csv_path) # Yield an error"
      ],
      "metadata": {
        "id": "lrraIedbrAxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Attempt with ISO-8859-1 encoding\n",
        "df = pd.read_csv(csv_path, encoding=\"ISO-8859-1\") # Or type: encoding=\"latin-1\" / encoding=\"Windows-1252\"\n",
        "df.head()"
      ],
      "metadata": {
        "id": "H-HkktR8rB3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "Ewc895H1rC-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check data types\n",
        "df.info()"
      ],
      "metadata": {
        "id": "PMfDZl3_rEFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a bar plot with value counts\n",
        "sns.countplot(x='Rating', data=df)"
      ],
      "metadata": {
        "id": "IEpfzUsqrFMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Part 2. Processing Text Data**"
      ],
      "metadata": {
        "id": "W-3kP2Z-rXTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NLP libraries\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator"
      ],
      "metadata": {
        "id": "7oqny6ifrgPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "I0lly4KDrhRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are a few additions to in earlier notebooks:\n",
        "* Dealing with contractions\n",
        "* Extending the stop word set by adding contextual terms"
      ],
      "metadata": {
        "id": "FkHgjuwVrjB0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Considering contractions in English**"
      ],
      "metadata": {
        "id": "b3n1eJShrk8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A dictionary of main contractions in English\n",
        "contractions = {\n",
        "\"ain't\": \"am not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how is\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"needn't\": \"need not\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there had\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we would\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who's\": \"who is\",\n",
        "\"won't\": \"will not\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"you'd\": \"you would\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you're\": \"you are\",\n",
        "\"you've\": \"you have\"\n",
        "}"
      ],
      "metadata": {
        "id": "oaNkKUr1rmD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extending the stop word set by adding contextual terms**"
      ],
      "metadata": {
        "id": "Twtq6XNcrxHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a basic stop word set\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "9MPbYgLIryBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extend the stop word set\n",
        "stop_words.update(['park', 'disney', 'disneyland']) # Context-specific stopwords"
      ],
      "metadata": {
        "id": "OsDyh4JtrzCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define a text processing function**"
      ],
      "metadata": {
        "id": "yO2lRFFzr0bQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bring lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "rEeMDa7Qr1Wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a text pre-processing function\n",
        "def process_text(text):\n",
        "    # Lowercasing\n",
        "    text = text.lower()\n",
        "\n",
        "    # Expand contractions\n",
        "    if True:\n",
        "        text = text.split()\n",
        "        new_text = []\n",
        "        for word in text:\n",
        "            if word in contractions:\n",
        "                new_text.append(contractions[word])\n",
        "            else:\n",
        "                new_text.append(word)\n",
        "        text = \" \".join(new_text)\n",
        "\n",
        "    # Remove punctuation and special characters\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\'', ' ', text)\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords & perform lemmatization\n",
        "    filtered_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "\n",
        "    return ' '.join(filtered_tokens)"
      ],
      "metadata": {
        "id": "JALwBufYr2eD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the pre-processing function\n",
        "df['Review_Clean'] = df['Review_Text'].apply(process_text)\n",
        "df['Review_Clean']"
      ],
      "metadata": {
        "id": "fXuis5-vsMd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Join text together\n",
        "review_words = ','.join(list(df['Review_Clean'].values))\n",
        "\n",
        "# Count each word\n",
        "Counter = Counter(review_words.split())\n",
        "most_frequent = Counter.most_common(30)\n",
        "\n",
        "# Bar plot of frequent words\n",
        "fig = plt.figure(1, figsize = (20,10))\n",
        "_ = pd.DataFrame(most_frequent, columns=(\"words\",\"count\"))\n",
        "sns.barplot(x = 'words', y = 'count', data = _, palette = 'winter')\n",
        "plt.xticks(rotation=45);"
      ],
      "metadata": {
        "id": "0mkX8555sN0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the word cloud\n",
        "wordcloud = WordCloud(background_color=\"white\",\n",
        "                      max_words= 200,\n",
        "                      contour_width = 8,\n",
        "                      contour_color = \"steelblue\",\n",
        "                      collocations=False).generate(review_words)\n",
        "\n",
        "# Visualize the word cloud\n",
        "fig = plt.figure(1, figsize = (10, 10))\n",
        "plt.axis('off')\n",
        "plt.imshow(wordcloud)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "W2WRGx0CsPPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Part 3. Sentiment Analysis**"
      ],
      "metadata": {
        "id": "sxtD5o1us0gJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **3.1. Predict text sentiment with the `TextBlob` library**\n",
        "* **Type:** Rule-based sentiment analysis.\n",
        "* **Features:** Returns polarity (negative/positive) and subjectivity (fact/opinion).\n",
        "\n"
      ],
      "metadata": {
        "id": "O8xLG0GAtOh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from textblob import TextBlob"
      ],
      "metadata": {
        "id": "yr5fsKJgs9k6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get sentiment scores using TextBlob\n",
        "def get_textblob_sentiment(text):\n",
        "    analysis = TextBlob(text)\n",
        "    return analysis.sentiment.polarity  # Returns a score between -1 (negative) and 1 (positive)"
      ],
      "metadata": {
        "id": "R-hNBl9_tASJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply sentiment analysis function to the processed text\n",
        "df['TextBlob_Sentiment'] = df['Review_Clean'].astype(str).apply(get_textblob_sentiment)"
      ],
      "metadata": {
        "id": "xDl2BfMGtAL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the updated DataFrame\n",
        "df"
      ],
      "metadata": {
        "id": "jyy8IMuftAED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explore the sentiment variable**"
      ],
      "metadata": {
        "id": "vkZs6wmntYoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary statistics\n",
        "df['TextBlob_Sentiment'].describe()"
      ],
      "metadata": {
        "id": "Vo1TaEUvtkgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot histogram for the 'TextBlob_Sentiment' column\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(df['TextBlob_Sentiment'], bins=30, edgecolor='black', alpha=0.7)\n",
        "plt.xlabel('Sentiment Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of TextBlob Sentiment Scores')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "f_bZ4V-at5jS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **3.2. Predict text sentiment with DistilBERT (Hugging Face)**\n",
        "* **Type:** Transformer-based deep learning model\n",
        "* **Features:** Provides sentiment class probabilities (e.g., `positive`, `neutral`, `negative`)."
      ],
      "metadata": {
        "id": "_rg7jWk-usKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "PDoyHHEhviJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize DistilBERT sentiment analysis pipeline\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\")"
      ],
      "metadata": {
        "id": "u1_c5Z0-vkUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply sentiment analysis to the processed text column (it takes soooooooooo long...)\n",
        "df['DistilBERT_Sentiment'] = df['Review_Clean'].astype(str).apply(lambda text: sentiment_pipeline(text)[0]['label'])"
      ],
      "metadata": {
        "id": "y_yZ6__8vkSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the updated DataFrame\n",
        "df"
      ],
      "metadata": {
        "id": "rNfXay3DvkPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Part 4. Emotion Features**"
      ],
      "metadata": {
        "id": "aaPn4Fbexx0_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **4.1. Predict emotions with NRC Emotion Lexicon**\n",
        "* **Type:** Lexicon-based emotion detection.\n",
        "* **Features:** Labels text with eight emotions (anger, fear, anticipation, trust, surprise, sadness, joy, disgust).\n"
      ],
      "metadata": {
        "id": "LmTgfwGZxq-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the NRC Emotion Lexicon if not already available\n",
        "!pip install nrclex"
      ],
      "metadata": {
        "id": "Jox6Yv8EyXlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from nrclex import NRCLex"
      ],
      "metadata": {
        "id": "26YcYHIqyZZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract emotion scores using NRC Emotion Lexicon\n",
        "def get_emotions(text):\n",
        "    emotion = NRCLex(text)\n",
        "    return emotion.raw_emotion_scores  # Returns a dictionary of emotion scores"
      ],
      "metadata": {
        "id": "z1vCdSg5yWqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the emotion analysis function to the processed text column\n",
        "df['NRC_Emotions'] = df['Review_Clean'].astype(str).apply(get_emotions)\n",
        "df['NRC_Emotions']"
      ],
      "metadata": {
        "id": "eO9mZl_PydrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert emotion dictionaries into separate columns\n",
        "emotion_df = df['NRC_Emotions'].apply(pd.Series).fillna(0)"
      ],
      "metadata": {
        "id": "fWkMAeJIyjAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge the emotion features into the original DataFrame\n",
        "df = pd.concat([df, emotion_df], axis=1)\n",
        "df"
      ],
      "metadata": {
        "id": "FywdSiO3ynh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explore the updated DataFrame `df`**"
      ],
      "metadata": {
        "id": "u8lXGHjhzEpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "t8G3FzB_zHmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Part 5: Predictive Analysis with Features**"
      ],
      "metadata": {
        "id": "bS41188zzlkN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Dependent variable: `Rating`\n",
        "* Independent variable: `TextBlob_Sentiment`, `anger`, `fear`, `anticipation`, `trust`, `surprise`, `sadness`, `joy`, `disgust`"
      ],
      "metadata": {
        "id": "9-x0YDjlzrjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
      ],
      "metadata": {
        "id": "pBiFLtZK0nce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define dependent and independent variables\n",
        "dependent_var = 'Rating'\n",
        "independent_vars = ['TextBlob_Sentiment', 'anger', 'fear', 'anticipation', 'trust',\n",
        "                    'surprise', 'sadness', 'joy', 'disgust']"
      ],
      "metadata": {
        "id": "GpVe2KkL0mbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop missing values to ensure complete cases\n",
        "df = df.dropna(subset=[dependent_var] + independent_vars)"
      ],
      "metadata": {
        "id": "P35bFdhB0sD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training (70%) and testing (30%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df[independent_vars], df[dependent_var],\n",
        "                                                    test_size=0.3, random_state=123)"
      ],
      "metadata": {
        "id": "SVNGemn60uGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train the linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "R0aeS5mr0v-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explore coefficients** (you don't need to understand the whole lines)"
      ],
      "metadata": {
        "id": "B1zRvBSj2dFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Get coefficients and intercept\n",
        "coefficients = model.coef_\n",
        "intercept = model.intercept_\n",
        "\n",
        "# Compute predictions and residuals\n",
        "y_pred = model.predict(X_train)\n",
        "residuals = y_train - y_pred\n",
        "\n",
        "# Compute standard errors\n",
        "n = X_train.shape[0]  # Number of observations\n",
        "p = X_train.shape[1]  # Number of predictors\n",
        "X_with_const = np.c_[np.ones(n), X_train]  # Add constant for intercept\n",
        "var_residuals = np.sum(residuals**2) / (n - p - 1)\n",
        "cov_matrix = np.linalg.inv(X_with_const.T @ X_with_const) * var_residuals\n",
        "std_errors = np.sqrt(np.diag(cov_matrix))\n",
        "\n",
        "# Compute t-statistics and p-values\n",
        "t_stats = coefficients / std_errors[1:]  # Exclude intercept from std_errors\n",
        "p_values = [2 * (1 - stats.t.cdf(np.abs(t), df=n - p - 1)) for t in t_stats]\n",
        "\n",
        "# Create DataFrame for results\n",
        "regression_results = pd.DataFrame({\n",
        "    'Variable': ['Intercept'] + independent_vars,\n",
        "    'Coefficient': [intercept] + list(coefficients),\n",
        "    'Standard Error': std_errors,\n",
        "    't-Statistic': [intercept / std_errors[0]] + list(t_stats),\n",
        "    'p-Value': [2 * (1 - stats.t.cdf(np.abs(intercept / std_errors[0]), df=n - p - 1))] + p_values\n",
        "})"
      ],
      "metadata": {
        "id": "CsEV_iyw1-eF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regression_results"
      ],
      "metadata": {
        "id": "a6r0D9m63FNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explore predictive performance**"
      ],
      "metadata": {
        "id": "UQDF9ruF2aM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on both training and test sets\n",
        "y_train_pred = model.predict(X_train)\n",
        "y_test_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "VUhazB9i0xbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute metrics for training set\n",
        "train_r2 = r2_score(y_train, y_train_pred)\n",
        "train_adj_r2 = 1 - (1-train_r2) * (len(y_train)-1) / (len(y_train)-X_train.shape[1]-1)\n",
        "train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
        "\n",
        "# Compute metrics for test set\n",
        "test_r2 = r2_score(y_test, y_test_pred)\n",
        "test_adj_r2 = 1 - (1-test_r2) * (len(y_test)-1) / (len(y_test)-X_test.shape[1]-1)\n",
        "test_mse = mean_squared_error(y_test, y_test_pred)\n",
        "test_mae = mean_absolute_error(y_test, y_test_pred)"
      ],
      "metadata": {
        "id": "k0GDPkL91EE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame to display results\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Metric': ['R-squared', 'Adjusted R-squared', 'Mean Squared Error', 'Mean Absolute Error'],\n",
        "    'Training Set': [train_r2, train_adj_r2, train_mse, train_mae],\n",
        "    'Test Set': [test_r2, test_adj_r2, test_mse, test_mae]\n",
        "})\n",
        "\n",
        "metrics_df"
      ],
      "metadata": {
        "id": "KYZoEQbX1F8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LF-fIcISAD1d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}