{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **#02: Text Parsing, Filtering, and Quantitative Representation**\n",
        "- Instructor: [Jaeung Sim](https://jaeungs.github.io/) (University of Connecticut)\n",
        "- Course: OPIM 5671: Data Mining and Time Series Forecasting\n",
        "- Last updated: September 17, 2025"
      ],
      "metadata": {
        "id": "Sk6TMPWUqn3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objectives**\n",
        "1. Explore real-world text data using `nltk` library.\n",
        "1. Understand the text processing procedure.\n",
        "\n",
        "**Contents**\n",
        "* Part 1: Understanding the Data\n",
        "* Part 2: Text Parsing and Filtering\n",
        "* Part 3: Singular Value Decomposition\n",
        "\n",
        "**References**\n",
        "* [YouTube Comments Dataset at Kaggle (Data Source)](https://www.kaggle.com/datasets/atifaliak/youtube-comments-dataset/data)\n",
        "* [Natural Language Toolkit (NLTK)](https://www.nltk.org/)"
      ],
      "metadata": {
        "id": "9ogQi1iMq9Ti"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Part 1: Understanding the Data**"
      ],
      "metadata": {
        "id": "TzwKegxRtcMz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Introduction to the Dataset**\n",
        "* **Source:** YouTube Comments Dataset at Kaggle (<https://www.kaggle.com/datasets/atifaliak/youtube-comments-dataset>)\n",
        "* **About this file**\n",
        "  * Introducing the `Youtube Comments Dataset.csv`, a fully cleaned and preprocessed collection of YouTube video comments. This dataset is ideal for sentiment analysis, natural language processing, and text-based machine learning projects. With all irrelevant data already removed and cleaning steps thoroughly performed, it provides clean, structured information, allowing you to focus solely on insights and analysis."
      ],
      "metadata": {
        "id": "ZE3ttzCMtjkC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Download data with Python codes**"
      ],
      "metadata": {
        "id": "jtTLZ1Jwsr2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import kagglehub\n",
        "import os"
      ],
      "metadata": {
        "id": "GAkWdKmZ6HDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkC8tJT3qjfy"
      },
      "outputs": [],
      "source": [
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"atifaliak/youtube-comments-dataset\")\n",
        "\n",
        "# Print the dataset path\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deal with DataFrame**"
      ],
      "metadata": {
        "id": "9RmYwrSE5d3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List files in the downloaded dataset directory\n",
        "files = os.listdir(path)\n",
        "print(\"Files in dataset:\", files)\n",
        "\n",
        "# Load the CSV file (assuming there's only one CSV file)\n",
        "csv_file = [f for f in files if f.endswith('.csv')][0]  # Get the first CSV file\n",
        "csv_path = os.path.join(path, csv_file)"
      ],
      "metadata": {
        "id": "xDmJ-7Dh4g5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read into DataFrame\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Display the first few rows\n",
        "df.head()"
      ],
      "metadata": {
        "id": "_PYWBwln6UtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "QtQVj37O_1Ti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop missing values\n",
        "df = df.dropna(subset=['Comment'])\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GvoUcduH_624"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check data types\n",
        "df.info()"
      ],
      "metadata": {
        "id": "G9713BgM_3dH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Part 2: Text Parsing and Filtering**"
      ],
      "metadata": {
        "id": "G4Qt2zHPAK47"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import libraries and download necessary NLTK resources**"
      ],
      "metadata": {
        "id": "tVO-dNfiAeeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "HsoKXm2HA5PR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "WxAmdkGWA6Wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 1: Text Parsing (Tokenization)**"
      ],
      "metadata": {
        "id": "H6ydAVDvAas4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Tokenizes the text into words using `word_tokenize()`.\n",
        "\n"
      ],
      "metadata": {
        "id": "1NgQ8rvlICVJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization**"
      ],
      "metadata": {
        "id": "YgGHht8CC8Tb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure text type\n",
        "df['Comment'] = df['Comment'].astype(str)"
      ],
      "metadata": {
        "id": "guwaM6wMBphe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the comment column\n",
        "def tokenize_text(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "df['tokenized'] = df['Comment'].apply(tokenize_text)"
      ],
      "metadata": {
        "id": "L8kMkRZgBG5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore the tokenized column\n",
        "df['tokenized']"
      ],
      "metadata": {
        "id": "WEOc0Co7B5_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Zipf's Law: Explore the distribution before filtering**"
      ],
      "metadata": {
        "id": "PJA_2qDpC_EM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from itertools import chain\n",
        "\n",
        "# Flatten the list of tokenized words\n",
        "all_tokens = list(chain.from_iterable(df['tokenized']))\n",
        "\n",
        "# Count word frequencies\n",
        "word_freq = Counter(all_tokens)\n",
        "\n",
        "# Get the top 100 most common words\n",
        "top_100_words = word_freq.most_common(100)\n",
        "\n",
        "# Separate words and their frequencies for plotting\n",
        "words, frequencies = zip(*top_100_words)\n",
        "\n",
        "# Plot the frequency distribution\n",
        "plt.figure(figsize=(15, 6))\n",
        "plt.bar(words, frequencies)\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Top 100 Most Frequent Terms in df[\"tokenized\"]')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sFfLvADVCSHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 2: Text Filtering**"
      ],
      "metadata": {
        "id": "m3kN_7TUCJ5t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Converts text to lowercase.\n",
        "* Removes punctuation and special characters.\n",
        "* Tokenizes the text into words.\n",
        "* Removes stopwords (like \"the\", \"is\", \"and\").\n",
        "* Applies lemmatization (converts words to their base form, e.g., \"running\" â†’ \"run\").\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F2jz_cZUH2I0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Basic pre-processing**"
      ],
      "metadata": {
        "id": "50aBvdl0EraV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a stopword set\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "9URNVBwNDlmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bring lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "ZjIULyS8DpUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a text pre-processing function\n",
        "def preprocess_text(text):\n",
        "    # Lowercasing\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation and special characters\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords & perform lemmatization\n",
        "    filtered_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "\n",
        "    return ' '.join(filtered_tokens)"
      ],
      "metadata": {
        "id": "iwnaGSGvDu3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the pre-processing function\n",
        "df['processed'] = df['Comment'].apply(preprocess_text)\n",
        "df['processed']"
      ],
      "metadata": {
        "id": "PY7YSjzgEkzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check the distribution after basic pre-processing**"
      ],
      "metadata": {
        "id": "G9XAPho4ExQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from itertools import chain\n",
        "\n",
        "# Flatten the list of tokenized words\n",
        "all_tokens = list(chain.from_iterable(df['processed'].apply(tokenize_text)))\n",
        "\n",
        "# Count word frequencies\n",
        "word_freq = Counter(all_tokens)\n",
        "\n",
        "# Get the top 100 most common words\n",
        "top_100_words = word_freq.most_common(100)\n",
        "\n",
        "# Separate words and their frequencies for plotting\n",
        "words, frequencies = zip(*top_100_words)\n",
        "\n",
        "# Plot the frequency distribution\n",
        "plt.figure(figsize=(15, 6))\n",
        "plt.bar(words, frequencies)\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Top 100 Most Frequent Terms in df[\"processed\"]')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GukCjBUIExp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Term Weighting (TF-IDF)**"
      ],
      "metadata": {
        "id": "MmeQJueFFrSX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Converts the processed text into **TF-IDF vectors**, assigning importance weights to words."
      ],
      "metadata": {
        "id": "2wgEecvIILDD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see the number of unique tokens before applying TF-IDF."
      ],
      "metadata": {
        "id": "6F8yhjGTHIL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure necessary libraries are imported\n",
        "from itertools import chain\n",
        "\n",
        "# Flatten the list of tokenized words and count unique tokens\n",
        "unique_tokens = set(chain.from_iterable(df['processed'].apply(tokenize_text)))\n",
        "num_unique_tokens = len(unique_tokens)\n",
        "\n",
        "# Display the number of unique tokens\n",
        "num_unique_tokens"
      ],
      "metadata": {
        "id": "psk6UmKuGocF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's apply TF-IDF and vectorize the corpus."
      ],
      "metadata": {
        "id": "wPZHY6zoHORl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize the processed data with TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_tfidf = vectorizer.fit_transform(df['processed'])"
      ],
      "metadata": {
        "id": "0Cb5Dnn1FvQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now it's a sparse term-document matrix\n",
        "X_tfidf"
      ],
      "metadata": {
        "id": "s0NgUYh3F8ks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given that you already removed stop words, the number of unique tokens merely reduced from 36788 to 36659 for now."
      ],
      "metadata": {
        "id": "sTvJqn4eHRDG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 3: Singular Value Decomposition (SVD)**"
      ],
      "metadata": {
        "id": "J8vRZkIpIjtR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Reduces dimensionality by selecting a limited number of features (default max: 100)\n",
        "* The result is stored in `df_svd`, which represents the most important textual features."
      ],
      "metadata": {
        "id": "09iE4LYBJBgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine the number of components to retain (e.g., 100 components)\n",
        "n_components = min(100, X_tfidf.shape[1])  # Keep max 100 or total features\n",
        "\n",
        "svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
        "X_svd = svd.fit_transform(X_tfidf)"
      ],
      "metadata": {
        "id": "h3BSvSIEJKiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert SVD results to DataFrame\n",
        "df_svd = pd.DataFrame(X_svd, columns=[f\"feature_{i+1}\" for i in range(n_components)])"
      ],
      "metadata": {
        "id": "Q3FkVGQhJLiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display results\n",
        "df_svd"
      ],
      "metadata": {
        "id": "NqBhjCyrJ1gV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary statistics\n",
        "df_svd.describe().T # Transpose (`T`) for convenience"
      ],
      "metadata": {
        "id": "3hrmbphyKJGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "98NLw99bmL2i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}